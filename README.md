# AI Agent Observatory
*A benchmarking and evaluation platform for analysing AI agents across accuracy, hallucination, cost, latency, and task success.*

---

## ğŸŒ Overview
The AI Agent Observatory is a research and engineering project designed to measure and compare the performance of different AI agent frameworks.

It provides a unified environment for evaluating:
- CrewAI agents  
- AutoGen agents  
- LangGraph workflows  
- Custom agent implementations  

The system focuses on producing transparent, reproducible, and comparable performance metrics.

This is part of a 3-project AI portfolio demonstrating advanced engineering and innovation for the UK Global Talent technical track.

---

## ğŸ¯ Problem Statement
AI agents behave differently depending on:
- framework architecture  
- prompting strategy  
- memory handling  
- tool execution  
- state management  

However, **there is currently no standardised way to evaluate agent quality**.

This makes it difficult for organisations to:
- choose the right agent framework  
- understand trade-offs  
- compare agents on reliability  
- determine hallucination risk  
- estimate cost or latency  

---

## ğŸš€ Solution
The AI Agent Observatory provides:
- A common evaluation dataset  
- Standard ground truth answers  
- Objective scoring metrics  
- Cost tracking  
- Latency measurement  
- Structured results for dashboards  

This enables fair comparison between autonomous agent systems.

---

## ğŸ§© Key Features

### âœ” Multi-Framework Evaluation
Supports:
- CrewAI  
- AutoGen  
- LangGraph  
- Custom agents  

### âœ” Performance Metrics
Evaluates:
- Accuracy  
- Hallucination rate  
- Task completion success  
- Cost per run  
- Response time  

### âœ” Dataset-Driven Testing
Each agent is tested on the same curated dataset:
- extraction tasks  
- summarisation tasks  
- reasoning tasks  
- fact-check tasks  
- JSON compliance tasks  

### âœ” Dashboard (Planned)
Visual comparisons of:
- accuracy trends  
- hallucination detection  
- cost distributions  
- agent ranking  
- latency scatter plots  

---

## ğŸ— Architecture

### High-Level Architecture  
Image stored at:

### Mermaid Diagram  

---

## ğŸ§  Technical Stack

| Layer        | Technology                            |
|--------------|----------------------------------------|
| Backend      | FastAPI / Python                       |
| Evaluation   | Custom scoring engine                  |
| Frameworks   | CrewAI, AutoGen, LangGraph             |
| LLMs         | OpenAI / Anthropic                     |
| Storage      | JSON datasets / PostgreSQL (planned)   |
| Frontend     | React dashboard (planned)              |

---

## ğŸ”¬ Research Notes
Full technical evaluation logic is documented in:

Includes:
- scoring approaches  
- hallucination detection logic  
- latency measurement  
- cost tracking  
- dataset structure  
- validation rules  

---

## ğŸ›  Installation (Coming Soon)
Additional setup instructions will be added during early development.

---

## ğŸ“… Roadmap

### Phase 1 â€” Evaluation Engine
- [x] Architecture design  
- [x] Research notes  
- [ ] Evaluation runner  
- [ ] Accuracy scoring  
- [ ] JSON validation tools  

### Phase 2 â€” Dataset  
- [ ] Test case library  
- [ ] Ground truth store  
- [ ] Multi-task benchmarks  

### Phase 3 â€” Framework Integrations
- [ ] CrewAI adapter  
- [ ] AutoGen adapter  
- [ ] LangGraph adapter  

### Phase 4 â€” Dashboard
- [ ] React interface  
- [ ] Charts and heatmaps  
- [ ] Leaderboard  

---

## ğŸ’¡ Why This Project Matters (For Assessors)
This project demonstrates:

- **Innovation**: evaluating autonomous AI agents  
- **Technical depth**: building scoring, metrics, hallucination detection  
- **Advanced engineering**: multi-framework evaluation  
- **Research capability**: designing test datasets + metrics  
- **Public value**: knowledge for the wider AI ecosystem  

Supports:
- **Mandatory Criterion (Innovation)**  
- **Optional Criterion (Technical Contribution)**  

---

## ğŸ“œ License
MIT License.
