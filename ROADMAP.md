# Roadmap – AI Agent Observatory

## Phase 1 — Foundations
- [x] Repo created
- [x] Architecture diagram added
- [x] Research notes added
- [ ] Define test case folders
- [ ] `.env.example` + config templates

## Phase 2 — Evaluation Dataset
- [ ] Create extraction tasks dataset
- [ ] Create summarisation dataset
- [ ] Create reasoning dataset
- [ ] Create fact-checking dataset
- [ ] Create JSON-format evaluation tasks

## Phase 3 — Agent Framework Adapters
- [ ] CrewAI adapter
- [ ] AutoGen adapter
- [ ] LangGraph adapter
- [ ] Unified interface for evaluation runs

## Phase 4 — Evaluation Engine
- [ ] Scoring algorithms (accuracy, hallucination)
- [ ] Cost computation (token tracking)
- [ ] Latency measurement
- [ ] Ground truth comparators
- [ ] JSON validation tools

## Phase 5 — Reporting & Outputs
- [ ] Structured JSON result output
- [ ] CSV result export
- [ ] Evaluation result storage

## Phase 6 — Dashboard (React)
- [ ] Accuracy charts
- [ ] Radar comparison views
- [ ] Cost vs performance
- [ ] Hallucination scatter plots
- [ ] Agent leaderboard

## Phase 7 — Testing
- [ ] Unit tests for scoring functions
- [ ] Integration tests across frameworks
- [ ] Add example evaluation run

## Phase 8 — Deployment
- [ ] Dockerfile
- [ ] Backend deployment
- [ ] Dashboard deployment
